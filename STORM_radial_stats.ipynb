{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743854e-3ccc-4f6b-9790-8f2597566efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "import dask\n",
    "import dask.delayed as delayed\n",
    "from dask.distributed import Client\n",
    "from glob import glob\n",
    "import track_analysis\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93c017-5c55-46e5-aa70-094896f17517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES\n",
    "dir_input = '../STORM'\n",
    "dir_output = './output'\n",
    "\n",
    "# List of models to analyse. Model data read as {dir_input}/{model_name}/*{basin}*\n",
    "model_list = ['IBTRACS','CMCC','HADGEM','ECEARTH','CNRM']\n",
    "\n",
    "# STORM basin from which to read input files\n",
    "basin = 'NA' # NA, SP, WP, EI or WI\n",
    "\n",
    "# ANALYSIS GRID SETTINGS\n",
    "resolution = 1/4 # Grid resolution in degrees\n",
    "lonmin = -107  # Grid minimum longitude\n",
    "lonmax = 2  # Grid maximum longitude\n",
    "latmin = 0  # Grid minimum latitude\n",
    "latmax = 63  # Grid maxmimum latitude\n",
    "margin = 3  # Margin around grid within which to generate storms (degrees)\n",
    "\n",
    "# Example (lonmin, lonmax, latmin, latmax) for basins\n",
    "# NA: (-107, 2, 63, 3)\n",
    "# SP: ()\n",
    "# WP: ()\n",
    "# EI: ()\n",
    "# WI: ()\n",
    "\n",
    "# ANALYSIS SETTINGS\n",
    "radius = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee711d-88b0-4e24-8b80-4be169874df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_analysis( df_idx_list, margin, lonmin, \n",
    "                       lonmax, latmin, latmax, resolution, \n",
    "                       radius):\n",
    "\n",
    "    n_idx = len(df_idx_list)\n",
    "    output_data = []\n",
    "\n",
    "    for kk in range(n_idx):\n",
    "        out_single = single_analysis( df_idx_list[kk], margin, \n",
    "                                      lonmin, lonmax, latmin, latmax, resolution,\n",
    "                                      radius = radius)\n",
    "        if out_single is None:\n",
    "            continue\n",
    "        else:\n",
    "            ds = out_single\n",
    "            output_data.append(ds)\n",
    "    \n",
    "    ds_concat = xr.concat( output_data, dim='storm' )\n",
    "    ds_out = xr.Dataset()\n",
    "    for cat in [0, 1, 2, 3, 4, 5]:\n",
    "        ds_tmp = ds_concat.data == cat\n",
    "        ds_tmp = ds_tmp.sum( dim='storm' )\n",
    "        ds_out[f'category_{cat}'] = ds_tmp\n",
    "\n",
    "    return ds_out\n",
    "\n",
    "def single_analysis(df_ii, margin, \n",
    "                    lonmin, lonmax, latmin, latmax, \n",
    "                    resolution, dir_tmp = './tmp', \n",
    "                    radius=200):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "\n",
    "    grid_lon = np.arange(lonmin, lonmax, resolution)\n",
    "    grid_lat = np.arange(latmin, latmax, resolution)\n",
    "    lon2, lat2 = np.meshgrid(grid_lon, grid_lat)\n",
    "    lonF = lon2.flatten()\n",
    "    latF = lat2.flatten()\n",
    "    \n",
    "    prdist = track_analysis.track_distance_to_box( df_ii, lonmin, lonmax, latmin, latmax )\n",
    "    n_r, n_c = lon2.shape\n",
    "    df_ii = df_ii[['longitude','latitude','category']]\n",
    "\n",
    "    if prdist > margin:\n",
    "        t = np.zeros((6, n_r, n_c))\n",
    "        return\n",
    "    else:\n",
    "        df_interp = track_analysis.interpolate_track( df_ii, delta=1/3 )\n",
    "        df_interp['category'] = df_interp['category'].round(0).astype(int)\n",
    "        \n",
    "        t = track_analysis.track_distance_to_grid( df_interp,\n",
    "                             lonF, latF,\n",
    "                             radius=radius )\n",
    "        t = t.reshape((6, n_r, n_c))\n",
    "        \n",
    "    for ii in range(6):\n",
    "        t[ii] = t[ii]*(ii+1)\n",
    "    t = np.max(t, axis = 0) - 1\n",
    "    \n",
    "    ds_out = xr.Dataset()\n",
    "    ds_out['x'] = (['x'], grid_lon)\n",
    "    ds_out['y'] = (['y'], grid_lat)\n",
    "    ds_out['data'] = (['y','x'], t)\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c38b1-90f0-4b11-91a7-0aa6d4d60a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over models in sepcified list\n",
    "for model_name in model_list:\n",
    "\n",
    "    # Get list of inputs and read into dataframe\n",
    "    fp_inputs = glob(f'{dir_input}/{model_name}/*{basin}*')\n",
    "    tracks_list = [ track_analysis.read_STORM( fp ) for fp in fp_inputs ]\n",
    "    tracks = pd.concat(tracks_list).reset_index()\n",
    "\n",
    "    # Separate tracks into individual events\n",
    "    tracks_events = track_analysis.separate_events( tracks )\n",
    "    n_events = len(tracks_events)\n",
    "\n",
    "    client = Client()\n",
    "    \n",
    "    pdel = delayed(multiple_analysis)\n",
    "    compute_list = []\n",
    "    batch_size = 50\n",
    "    for idx0 in np.arange(0, 250, batch_size):#np.arange(0,n_events,batch_size):\n",
    "        idx1 = idx0 + batch_size\n",
    "        tracks_ii_list = tracks_events[idx0:idx1]\n",
    "        compute_list.append( pdel( tracks_ii_list, margin, \n",
    "                                   lonmin, lonmax, latmin, latmax, \n",
    "                                   resolution, radius ) )\n",
    "    \n",
    "    out = dask.compute(compute_list, scheduler = 'processes')\n",
    "\n",
    "    ds_concat = xr.concat(out[0], dim='storm')\n",
    "    ds_total_count = ds_concat.sum(dim='storm').compute()\n",
    "    n_years = 10000\n",
    "    ds_count_per_year = ds_total_count / n_years\n",
    "\n",
    "    attrs = {'title': f'Average number of tropical cyclones of varying categories passing within {radius}km per year.',\n",
    "             'radius': f'{radius}km',\n",
    "             'years_analysed': f'{n_years}',\n",
    "             'track_dataset':'STORM',\n",
    "             'model':f'{model_name}'}\n",
    "    ds_count_per_year.attrs = attrs\n",
    "\n",
    "    fp_out_template = os.path.join( dir_output, 'annual_probabilities_STORM_{0}_{1}_200km.nc' )\n",
    "    ds_count_per_year.to_netcdf(fp_out_template.format(model_name, basin))\n",
    "\n",
    "    for fp in fp_to_concat:\n",
    "        os.remove(fp)\n",
    "\n",
    "    # Clean up\n",
    "    client.close()\n",
    "    del track_events\n",
    "    del tracks\n",
    "    del ds_tmp\n",
    "    del ds_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
